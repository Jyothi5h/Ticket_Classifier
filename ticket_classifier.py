# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NjeCxzwNQnquViI5j0IFCvqh5J5tyR8l

# Importing Necessary Libraries
"""

import pandas as pd
import numpy as np
import re
import string
import joblib
from textblob import TextBlob
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
import spacy
from spacy.matcher import PhraseMatcher
import json
import gradio as gr
from xgboost import XGBClassifier

"""# Data Pre-Processing"""

# reading the dataset
df = pd.read_excel("/content/drive/MyDrive/Colab Notebooks/ai_dev_assignment_tickets_complex_1000.xls")

df .head()

df.count()

print(df.isnull().sum())

# Handling the missing values
df.dropna(subset=['ticket_text', 'issue_type', 'urgency_level'], inplace=True)

print(df.isnull().sum())

# Loading spaCy to text preprocessing
nlp = spacy.load("en_core_web_sm")

# Defining preprocessing function
def preprocess_text(text):
    # Lowercase
    text = text.lower()

    # Removing special characters and extra whitespace
    text = re.sub(r'[^a-z0-9\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()

    # Processing text with spaCy
    doc = nlp(text)

    # Lemmatizing tokens
    tokens = [token.lemma_ for token in doc if not token.is_punct]

    return " ".join(tokens)
df['new_ticket_text'] = df['ticket_text'].apply(preprocess_text)

"""# Feature engineering"""

# feature extraction
tfidf_vectorizer = TfidfVectorizer(max_features=500)
bow_vectorizer = CountVectorizer(max_features=500)

# Generate TF-IDF and BOW features
tfidf_matrix = tfidf_vectorizer.fit_transform(df['new_ticket_text'])
bow_matrix = bow_vectorizer.fit_transform(df['new_ticket_text'])

#feature engineering
df['char_count'] = df['ticket_text'].apply(len)
df['word_count'] = df['new_ticket_text'].apply(lambda x: len(x.split()))
df['sentiment_score'] = df['ticket_text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)

additional_features = df[['char_count', 'word_count', 'sentiment_score']].reset_index(drop=True)
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
bow_df = pd.DataFrame(bow_matrix.toarray(), columns=bow_vectorizer.get_feature_names_out())

X_tfidf = pd.concat([additional_features, tfidf_df], axis=1)
X_bow = pd.concat([additional_features, bow_df], axis=1)

# Encoding the labels
le_issue = LabelEncoder()
le_urgency = LabelEncoder()
df['issue_type_encoded'] = le_issue.fit_transform(df['issue_type'])
df['urgency_level_encoded'] = le_urgency.fit_transform(df['urgency_level'])

y_issue = df['issue_type_encoded']
y_urgency = df['urgency_level_encoded']

"""# Specifying the models

"""

# Defining the models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(),
    'Naive Bayes': MultinomialNB(),
    'Support Vector Machine': SVC(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
}

"""# Issue and Urgency type classification with Bag of Words"""

X_train_bow_issue, X_test_bow_issue, y_issue_train, y_issue_test = train_test_split(X_bow, y_issue, test_size=0.2, random_state=42)
X_train_bow_urgency, X_test_bow_urgency, y_urgency_train, y_urgency_test = train_test_split(X_bow, y_urgency, test_size=0.2, random_state=42)

# Evaluating the models for Issue Type Classification (Bag-of-Words)
print("ISSUE TYPE CLASSIFICATION (Bag-of-Words)")
for name, model in models.items():
    try:
        model.fit(X_train_bow_issue, y_issue_train)
        preds = model.predict(X_test_bow_issue)
        report = classification_report(y_issue_test, preds, target_names=le_issue.classes_, output_dict=True)
        macro_f1 = report['macro avg']['f1-score']
        print(f"\nModel: {name}")
        print(classification_report(y_issue_test, preds, target_names=le_issue.classes_))
    except Exception as e:
        print(f"Model {name} failed: {e}")

# Evaluating the models for Urgency Level Classification (Bag-of-Words)
print("URGENCY LEVEL CLASSIFICATION (Bag-of-Words)")
for name, model in models.items():
    try:
        model.fit(X_train_bow_urgency, y_urgency_train)
        preds = model.predict(X_test_bow_urgency)
        report = classification_report(y_urgency_test, preds, target_names=le_urgency.classes_, output_dict=True)
        macro_f1 = report['macro avg']['f1-score']
        print(f"\nModel: {name}")
        print(classification_report(y_urgency_test, preds, target_names=le_urgency.classes_))
    except Exception as e:
        print(f"Model {name} failed: {e}")

"""# Issue and urgency type classification with TF-IDF"""

X_train_tfidf_urgency, X_test_tfidf_urgency, y_urgency_train, y_urgency_test = train_test_split(X_tfidf, y_urgency, test_size=0.2, random_state=42)
X_train_tfidf_issue, X_test_tfidf_issue, y_issue_train, y_issue_test = train_test_split(X_tfidf, y_issue, test_size=0.2, random_state=42)

# Evaluating the models for Issue Type Classification (TF-IDF)
print("ISSUE TYPE CLASSIFICATION (TF-IDF)")
for name, model in models.items():
    try:
        model.fit(X_train_tfidf_issue, y_issue_train)
        preds = model.predict(X_test_tfidf_issue)
        report = classification_report(y_issue_test, preds, target_names=le_issue.classes_, output_dict=True)
        macro_f1 = report['macro avg']['f1-score']
        print(f"\nModel: {name}")
        print(classification_report(y_issue_test, preds, target_names=le_issue.classes_))
    except Exception as e:
        print(f"Model {name} failed: {e}")

# Evaluating the models for Urgency Level Classification (TF-IDF)
print("URGENCY LEVEL CLASSIFICATION (TF-IDF)")
for name, model in models.items():
    try:
        model.fit(X_train_tfidf_urgency, y_urgency_train)
        preds = model.predict(X_test_tfidf_urgency)
        report = classification_report(y_urgency_test, preds, target_names=le_urgency.classes_, output_dict=True)
        macro_f1 = report['macro avg']['f1-score']
        print(f"\nModel: {name}")
        print(classification_report(y_urgency_test, preds, target_names=le_urgency.classes_))
    except Exception as e:
        print(f"Model {name} failed: {e}")

"""# Entity Extraction and prediction"""

X_train_bow_issue, X_test_bow_issue, y_issue_train, y_issue_test = train_test_split(X_bow, y_issue, test_size=0.2, random_state=42)
X_train_tfidf_urgency, X_test_tfidf_urgency, y_urgency_train, y_urgency_test = train_test_split(X_tfidf, y_urgency, test_size=0.2, random_state=42)

knn_issue_model = KNeighborsClassifier()
knn_issue_model.fit(X_train_bow_issue, y_issue_train)
joblib.dump(knn_issue_model, "best_issue_model_knn_bow.pkl")

knn_urgency_model = KNeighborsClassifier()
knn_urgency_model.fit(X_train_tfidf_urgency, y_urgency_train)
joblib.dump(knn_urgency_model, "best_urgency_model_knn_tfidf.pkl")

# Loading the model
best_issue_model = joblib.load("best_issue_model_knn_bow.pkl")
best_urgency_model = joblib.load("best_urgency_model_knn_tfidf.pkl")

vectorizer_issue = bow_vectorizer
vectorizer_urgency = tfidf_vectorizer

# Entity Extraction
def extract_entities(text):
    doc = nlp(text)
    product_list = df['product'].unique()
    matcher = PhraseMatcher(nlp.vocab, attr="LOWER")
    patterns = [nlp.make_doc(prod) for prod in product_list if prod != 'Unknown']
    matcher.add("PRODUCT", patterns)
    matches = matcher(doc)
    products = list(set([doc[start:end].text for match_id, start, end in matches]))

    dates = [ent.text for ent in doc.ents if ent.label_ == "DATE"]
    complaint_keywords = [token.text for token in doc if token.text.lower() in ['broken', 'late', 'error', 'delay', 'damaged', 'cancelled']]

    return {
        "products": products,
        "dates": dates,
        "complaint_keywords": complaint_keywords
    }

# defining the prediction Function
def predict_ticket(text):
    clean = preprocess_text(text)
    vector_tfidf = vectorizer_urgency.transform([clean])
    vector_bow = vectorizer_issue.transform([clean])
    char_count = len(text)
    word_count = len(clean.split())
    sentiment_score = TextBlob(text).sentiment.polarity
    extra_feat = pd.DataFrame([[char_count, word_count, sentiment_score]], columns=['char_count', 'word_count', 'sentiment_score'])

    final_issue = pd.concat([
        extra_feat, pd.DataFrame(vector_bow.toarray(), columns=vectorizer_issue.get_feature_names_out())
    ], axis=1)
    final_issue = final_issue.reindex(columns=X_train_bow_issue.columns, fill_value=0)
    issue_pred = le_issue.inverse_transform(best_issue_model.predict(final_issue))[0]

    final_urgency = pd.concat([
        extra_feat, pd.DataFrame(vector_tfidf.toarray(), columns=vectorizer_urgency.get_feature_names_out())
    ], axis=1)
    final_urgency = final_urgency.reindex(columns=X_train_tfidf_urgency.columns, fill_value=0)
    urgency_pred = le_urgency.inverse_transform(best_urgency_model.predict(final_urgency))[0]

    entities = extract_entities(text)
    return {
        "issue_type": issue_pred,
        "urgency_level": urgency_pred,
        "entities": entities
    }

"""# Using Gradio Interface"""

interface = gr.Interface(
    fn=lambda text: json.dumps(predict_ticket(text), indent=2),
    inputs=gr.Textbox(lines=6, placeholder="Enter support ticket text here..."),
    outputs=gr.Textbox(label="Prediction Output"),
    title="Ticket Classifier & Entity Extractor",
    description="Enter a support ticket text to predict issue type, urgency level, and extract relevant entities."
)

interface.launch()